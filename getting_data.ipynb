{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Black Lives Matter Uprisings Media Analysis\n",
    "\n",
    "This project attempts to answer through social natural language processing: How does mainstream media coverage of the 2020 Black Lives Matter uprisings following the deaths of Nina Pop, Breonna Taylor, George Floyd, Tony McDade, Ahmaud Arbery, and others perpetuate or interrupt anti-Blackness and carceral logics? In a critical moment in the movement, mainstream media has a large stake in framing uprisings to the general population and have historically been part of the perpetuation and platforming of anti-Black rhetoric and carceral logics. Exposing anti-Blackness and its ties to carceral logics allows for a way to not only deconstruct systems that do not serve Black communities in particular, but additionally allows for space for a conversation to imagine new ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing Data\n",
    "The dataset will be a corpus of articles scraped from major mainstream U.S. news sources, including the Associated Press, CNN, San Francisco Chronicle, Washington Post, and USA Today by searching “George Floyd protests” on each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "def parsing(articles_list, num, link_begin):\n",
    "\n",
    "    for n in np.arange(0, num):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = articles_list[n]['href']\n",
    "        link = link_begin + link\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = articles_list[n].find('h1').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='Article')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "def make_soup(url, html_marker, class_name):\n",
    "    r = requests.get(url)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content, 'html5lib')\n",
    "    articles = soup.find_all(html_marker, class_=class_name)\n",
    "    return articles, len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "apnews_art, apnews_len = make_soup('https://apnews.com/GeorgeFloyd', 'a', 'Component-headline-0-2-106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing(apnews_art, apnews_len, \"https://apnews.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of news_contents: 50\n",
      "len of list_links: 50\n",
      "len of news_titles: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"len of news_contents:\", len(news_contents))\n",
    "print(\"len of list_links:\", len(list_links))\n",
    "print(\"len of news_titles:\", len(list_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_soup(url):\n",
    "    with requests.Session() as req:\n",
    "        for item in range(1, 1000, 100):\n",
    "            r = req.get(url.format(item)).json()\n",
    "            for a in r['result']:\n",
    "                news_contents.append(a[\"body\"])\n",
    "                list_links.append(a[\"url\"])\n",
    "                list_titles.append(a[\"headline\"])\n",
    "                \n",
    "cnn_soup('https://search.api.cnn.io/content?q=george%20floyd%20protest&sort=newest&category=business,us,politics,world,opinion,health&size=100&from={}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of news_contents: 390\n",
      "len of list_links: 390\n",
      "len of news_titles: 390\n"
     ]
    }
   ],
   "source": [
    "print(\"len of news_contents:\", len(news_contents))\n",
    "print(\"len of list_links:\", len(list_links))\n",
    "print(\"len of news_titles:\", len(list_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='93befd90547344029350d2f06bf2f1ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfchron_articles = newsapi.get_everything(q=\"george floyd protests\",\n",
    "                                      domains='sfchronicle.com',\n",
    "                                      from_param='2020-05-29',\n",
    "                                      to='2020-06-28',\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=100,\n",
    "                                      page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfchron_links = []\n",
    "sfchron_titles = []\n",
    "for item in range(len(sfchron_articles['articles'])):\n",
    "    sfchron_links.append(sfchron_articles['articles'][item]['url'])\n",
    "    sfchron_titles.append(sfchron_articles['articles'][item]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(linkslst, titles, html_marker, marker_class):\n",
    "    \n",
    "    for i in np.arange(0, len(linkslst)):\n",
    "        link = linkslst[i]\n",
    "        \n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all(html_marker, class_=marker_class)\n",
    "        if len(body) > 0:\n",
    "            list_titles.append(titles[i])\n",
    "            list_links.append(link)\n",
    "            x = body[0].find_all('p')\n",
    "        \n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "            news_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content(sfchron_links, sfchron_titles, 'section', 'body')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of news_contents: 404\n",
      "len of list_links: 404\n",
      "len of news_titles: 404\n"
     ]
    }
   ],
   "source": [
    "print(\"len of news_contents:\", len(news_contents))\n",
    "print(\"len of list_links:\", len(list_links))\n",
    "print(\"len of news_titles:\", len(list_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wapo_articles = newsapi.get_everything(q=\"george floyd protests\",\n",
    "                                      domains='washingtonpost.com',\n",
    "                                      from_param='2020-05-29',\n",
    "                                      to='2020-06-28',\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=100,\n",
    "                                      page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wapo_links = []\n",
    "wapo_titles = []\n",
    "for item in range(len(wapo_articles['articles'])):\n",
    "    wapo_links.append(wapo_articles['articles'][item]['url'])\n",
    "    wapo_titles.append(wapo_articles['articles'][item]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content(wapo_links, wapo_titles, 'div', 'article-body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of news_contents: 501\n",
      "len of list_links: 501\n",
      "len of news_titles: 501\n"
     ]
    }
   ],
   "source": [
    "print(\"len of news_contents:\", len(news_contents))\n",
    "print(\"len of list_links:\", len(list_links))\n",
    "print(\"len of news_titles:\", len(list_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_articles = newsapi.get_everything(q=\"george floyd protests\",\n",
    "                                      domains='usatoday.com',\n",
    "                                      from_param='2020-05-29',\n",
    "                                      to='2020-06-28',\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=100,\n",
    "                                      page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_links = []\n",
    "usa_titles = []\n",
    "for item in range(len(usa_articles['articles'])):\n",
    "    usa_links.append(usa_articles['articles'][item]['url'])\n",
    "    usa_titles.append(usa_articles['articles'][item]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_content(usa_links, usa_titles, 'div', 'gnt_ar_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of news_contents: 590\n",
      "len of list_links: 590\n",
      "len of list_titles: 590\n"
     ]
    }
   ],
   "source": [
    "print(\"len of news_contents:\", len(news_contents))\n",
    "print(\"len of list_links:\", len(list_links))\n",
    "print(\"len of list_titles:\", len(list_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Beyoncé drops 'Black Parade' on Juneteenth, proceeds to benefit Black-owned businesses\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_titles[589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(list_titles, list_links, news_contents)), columns=['title', 'link', 'article_body'])\n",
    "df.to_csv('articles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
